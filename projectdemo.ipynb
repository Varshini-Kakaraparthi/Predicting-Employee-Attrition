{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MpLp9EAkAcOw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 22:26:17.341096: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-20 22:26:17.342127: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-20 22:26:17.344553: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-20 22:26:17.351000: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740070577.362607   33200 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740070577.365788   33200 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 22:26:17.378223: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-20 22:26:18.932830: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5560 - loss: 0.6876 - val_accuracy: 0.7966 - val_loss: 0.6027\n",
      "Epoch 2/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8486 - loss: 0.5603 - val_accuracy: 0.7966 - val_loss: 0.5205\n",
      "Epoch 3/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8344 - loss: 0.4763 - val_accuracy: 0.7966 - val_loss: 0.4733\n",
      "Epoch 4/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8284 - loss: 0.4313 - val_accuracy: 0.7966 - val_loss: 0.4602\n",
      "Epoch 5/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8620 - loss: 0.3615 - val_accuracy: 0.7966 - val_loss: 0.4544\n",
      "Epoch 6/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8507 - loss: 0.3516 - val_accuracy: 0.7966 - val_loss: 0.4507\n",
      "Epoch 7/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8499 - loss: 0.3474 - val_accuracy: 0.8093 - val_loss: 0.4398\n",
      "Epoch 8/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8595 - loss: 0.3432 - val_accuracy: 0.8178 - val_loss: 0.4323\n",
      "Epoch 9/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8648 - loss: 0.3236 - val_accuracy: 0.8305 - val_loss: 0.4274\n",
      "Epoch 10/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8776 - loss: 0.3131 - val_accuracy: 0.8559 - val_loss: 0.4186\n",
      "Epoch 11/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8845 - loss: 0.3005 - val_accuracy: 0.8517 - val_loss: 0.4137\n",
      "Epoch 12/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8730 - loss: 0.2971 - val_accuracy: 0.8517 - val_loss: 0.4104\n",
      "Epoch 13/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8928 - loss: 0.2864 - val_accuracy: 0.8517 - val_loss: 0.4106\n",
      "Epoch 14/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8919 - loss: 0.2752 - val_accuracy: 0.8432 - val_loss: 0.4026\n",
      "Epoch 15/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8853 - loss: 0.2738 - val_accuracy: 0.8559 - val_loss: 0.4057\n",
      "Epoch 16/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8888 - loss: 0.2643 - val_accuracy: 0.8517 - val_loss: 0.4000\n",
      "Epoch 17/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8885 - loss: 0.2643 - val_accuracy: 0.8432 - val_loss: 0.3950\n",
      "Epoch 18/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8973 - loss: 0.2663 - val_accuracy: 0.8432 - val_loss: 0.3961\n",
      "Epoch 19/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9167 - loss: 0.2266 - val_accuracy: 0.8432 - val_loss: 0.3993\n",
      "Epoch 20/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9032 - loss: 0.2681 - val_accuracy: 0.8475 - val_loss: 0.4015\n",
      "Epoch 21/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9183 - loss: 0.2300 - val_accuracy: 0.8390 - val_loss: 0.4012\n",
      "Epoch 22/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9091 - loss: 0.2467 - val_accuracy: 0.8432 - val_loss: 0.3974\n",
      "Epoch 23/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9176 - loss: 0.2330 - val_accuracy: 0.8432 - val_loss: 0.4022\n",
      "Epoch 24/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9155 - loss: 0.2264 - val_accuracy: 0.8432 - val_loss: 0.4000\n",
      "Epoch 25/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9057 - loss: 0.2325 - val_accuracy: 0.8432 - val_loss: 0.4069\n",
      "Epoch 26/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9145 - loss: 0.2168 - val_accuracy: 0.8432 - val_loss: 0.4088\n",
      "Epoch 27/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9221 - loss: 0.1969 - val_accuracy: 0.8432 - val_loss: 0.4044\n",
      "Epoch 28/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9329 - loss: 0.2000 - val_accuracy: 0.8432 - val_loss: 0.4103\n",
      "Epoch 29/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9195 - loss: 0.2296 - val_accuracy: 0.8390 - val_loss: 0.4096\n",
      "Epoch 30/30\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9417 - loss: 0.1864 - val_accuracy: 0.8390 - val_loss: 0.4141\n",
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "LSTM - Accuracy: 87.41%\n",
      "LSTM - Precision: 54.55%\n",
      "LSTM - Recall: 30.77%\n",
      "LSTM - F1-score: 39.34%\n",
      "XGBoost - Accuracy: 86.39%\n",
      "XGBoost - Precision: 44.44%\n",
      "XGBoost - Recall: 10.26%\n",
      "XGBoost - F1-score: 16.67%\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "Hybrid Model Performance:\n",
      "Hybrid - Accuracy: 88.78%\n",
      "Hybrid - Precision: 75.00%\n",
      "Hybrid - Recall: 23.08%\n",
      "Hybrid - F1-score: 35.29%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, InputLayer\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('HR_Analytics.csv')\n",
    "\n",
    "# Drop irrelevant or constant columns\n",
    "columns_to_drop = ['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeNumber']\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Encode target variable\n",
    "target_column = 'Attrition'\n",
    "label_encoder = LabelEncoder()\n",
    "df[target_column] = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "# Handle missing values appropriately\n",
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "categorical_cols = df.select_dtypes(exclude='number').columns\n",
    "\n",
    "# Fill missing values\n",
    "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.drop(target_column)\n",
    "\n",
    "# Encode categorical columns using OneHotEncoder\n",
    "for col in categorical_cols:\n",
    "    if df[col].dtype in [np.int64, np.float64]:\n",
    "        continue  # Skip if already numeric\n",
    "    if df[col].nunique() == 2:\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "    else:\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        encoded_data = onehot_encoder.fit_transform(df[[col]])\n",
    "        encoded_df = pd.DataFrame(encoded_data, columns=[f\"{col}_{cat}\" for cat in onehot_encoder.categories_[0]])\n",
    "        df = df.drop(col, axis=1).join(encoded_df)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(target_column, axis=1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "\n",
    "# Ensure all columns in X are numeric\n",
    "X = X.select_dtypes(include=['number'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare LSTM input (reshape for sequential model)\n",
    "X_train_lstm = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test_lstm = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# LSTM Model (using InputLayer)\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(InputLayer(shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n",
    "lstm_model.add(LSTM(50, activation='relu'))\n",
    "lstm_model.add(Dropout(0.3))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train LSTM\n",
    "lstm_model.fit(X_train_lstm, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# XGBoost Model with Hyperparameter Tuning using GridSearchCV\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "best_xgb_model = grid_search.best_estimator_  # Get the best XGBoost model\n",
    "\n",
    "# Evaluate Models\n",
    "def evaluate_model(model, X_test, y_test, model_type=\"LSTM\"):\n",
    "    if model_type == \"LSTM\":\n",
    "        X_test_reshaped = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "        y_pred = model.predict(X_test_reshaped)\n",
    "        y_pred = (y_pred > 0.5).astype(int)\n",
    "    elif model_type == \"Hybrid\":\n",
    "        y_pred = model(X_test)  # Call lambda function for Hybrid\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"{model_type} - Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"{model_type} - Precision: {precision * 100:.2f}%\")\n",
    "    print(f\"{model_type} - Recall: {recall * 100:.2f}%\")\n",
    "    print(f\"{model_type} - F1-score: {f1 * 100:.2f}%\")\n",
    "\n",
    "# Evaluate both models\n",
    "evaluate_model(lstm_model, X_test, y_test, \"LSTM\")\n",
    "evaluate_model(best_xgb_model, X_test, y_test, \"XGBoost\")  # Use the best XGBoost model\n",
    "\n",
    "# Hybrid Prediction (Averaging probabilities)\n",
    "lstm_prob = lstm_model.predict(X_test_lstm).flatten()\n",
    "xgb_prob = best_xgb_model.predict_proba(X_test)[:, 1]  # Use best_xgb_model\n",
    "\n",
    "hybrid_prob = (lstm_prob + xgb_prob) / 2\n",
    "hybrid_pred = (hybrid_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate Hybrid Model (using lambda function)\n",
    "print(\"\\nHybrid Model Performance:\")\n",
    "evaluate_model(lambda x: hybrid_pred, X_test, y_test, \"Hybrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RBxBbYLAAzWq"
   },
   "outputs": [],
   "source": [
    "# Predict attrition for new data\n",
    "def predict_attrition(new_employee_data):\n",
    "    new_employee_df = pd.DataFrame([new_employee_data])\n",
    "\n",
    "    # One-hot encode categorical features in new data\n",
    "    for col in categorical_cols:\n",
    "        if col in new_employee_df.columns:\n",
    "            if new_employee_df[col].nunique() == 2:\n",
    "                new_employee_df[col] = LabelEncoder().fit_transform(new_employee_df[col])\n",
    "            else:\n",
    "                # Get one-hot encoded columns from training data\n",
    "                onehot_cols = [c for c in X_train.columns if c.startswith(col + '_')]\n",
    "\n",
    "                # Create one-hot encoded columns for new data, filled with 0\n",
    "                for onehot_col in onehot_cols:\n",
    "                    new_employee_df[onehot_col] = 0\n",
    "\n",
    "                # Set the appropriate one-hot column to 1 based on the value in new data\n",
    "                value = new_employee_df[col][0]  # Get the value of the categorical feature\n",
    "\n",
    "\n",
    "                matching_col = col + '_' + str(value)\n",
    "\n",
    "                if matching_col in new_employee_df.columns:\n",
    "                    new_employee_df[matching_col] = 1\n",
    "\n",
    "    # Drop original categorical columns\n",
    "    new_employee_df = new_employee_df.drop(categorical_cols, axis=1, errors='ignore')\n",
    "    # Reindex to match the training data columns\n",
    "\n",
    "    new_employee_df = new_employee_df.reindex(columns=X_train.columns, fill_value=0)\n",
    "   # print(new_employee_df.columns)\n",
    "    # LSTM Prediction\n",
    "    lstm_input = new_employee_df.values.reshape((1, 1, X_train.shape[1]))\n",
    "    lstm_prediction = lstm_model.predict(lstm_input).flatten()[0]\n",
    "\n",
    "    # XGBoost Prediction (using best model)\n",
    "    xgb_prediction = best_xgb_model.predict_proba(new_employee_df)[:, 1][0]  # Use best_xgb_model\n",
    "\n",
    "    # Hybrid Prediction\n",
    "    hybrid_prediction = (lstm_prediction + xgb_prediction) / 2\n",
    "\n",
    "    if hybrid_prediction < 0.06:\n",
    "        print(\"Prediction: Employee is likely to leave.\")\n",
    "    else:\n",
    "        print(\"Prediction: Employee is likely to stay.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Muy4U2lZAeEV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['X_train.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained LSTM model\n",
    "lstm_model.save('lstm_model.h5')\n",
    "\n",
    "# Save the best XGBoost model\n",
    "joblib.dump(best_xgb_model, 'best_xgb_model.pkl')\n",
    "\n",
    "# Save the label encoder\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "\n",
    "# Save the standard scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "joblib.dump(X_train, \"X_train.pkl\")  # Save it for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oM8kVfnuAf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Prediction: Employee is likely to stay.\n"
     ]
    }
   ],
   "source": [
    "#No\n",
    "# Example usage with new_employee_data:\n",
    "new_employee_data = {\n",
    "    'Age': 49,\n",
    "    'Department': 'Research & Development',\n",
    "    'JobRole': 'Research Scientist',\n",
    "    'JobSatisfaction': 2,\n",
    "    'MaritalStatus': 'Married',\n",
    "    'MonthlyIncome': 5130,\n",
    "    'OverTime': 'NO',\n",
    "    'PercentSalaryHike': 23,\n",
    "    'PerformanceRating': 4,\n",
    "    'StockOptionLevel': 1,\n",
    "    'TotalWorkingYears': 10,\n",
    "    'WorkLifeBalance': 3,\n",
    "    'YearsAtCompany': 10,\n",
    "    'YearsInCurrentRole': 7,\n",
    "    'YearsSinceLastPromotion': 1,\n",
    "}\n",
    "\n",
    "predict_attrition(new_employee_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0NKRHh6iAmAV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Prediction: Employee is likely to leave.\n"
     ]
    }
   ],
   "source": [
    "#Yes\n",
    "# Example usage with new_employee_data:\n",
    "new_employee_data = {\n",
    "    'Age': 41,\n",
    "    'Department': 'Sales',\n",
    "    'JobRole': 'Sales Executive',\n",
    "    'JobSatisfaction': 4,\n",
    "    'MaritalStatus': 'Single',\n",
    "    'MonthlyIncome': 5993,\n",
    "    'OverTime': 'YES',\n",
    "    'PercentSalaryHike': 11,\n",
    "    'PerformanceRating': 3,\n",
    "    'StockOptionLevel': 0,\n",
    "    'TotalWorkingYears': 8,\n",
    "    'WorkLifeBalance': 1,\n",
    "    'YearsAtCompany': 6,\n",
    "    'YearsInCurrentRole': 4,\n",
    "    'YearsSinceLastPromotion': 0,\n",
    "}\n",
    "\n",
    "predict_attrition(new_employee_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
